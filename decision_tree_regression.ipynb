{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Algorithm From Scratch\n",
    "This short project was inspired by one of my projects at work. At some point this year, I found myself translating SAS script to Python script. One of my previous projects involved replicating a decision tree algorithm originally built in SAS Enterprise Miner into a Python script. So, in this project, I will be building a decision tree algorithm from scratch in Python, and to show this algorithm in action, I have applied it to a regression problem in this work book."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libaries\n",
    "Through out this work book, I have used just two popular libaries - numpy and pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "To implement the decision tree algorithm, I have a real dataset of house prices sold in Seattle, Washington, USA., between August and December 2022. The goal is to predict the average housing price of a home given some relevant features of a home.\n",
    "\n",
    "Relevant features considered for model building were: \n",
    "\n",
    "* beds: This is the number of bedrooms in a house \n",
    "* baths: This is the number of bathrooms in a house, where rows with 0.5 bath indicates a half bath, which translates to a bath with just the sink and water closet without a tub or shower.\n",
    "* size: This is the total floor area of a house\n",
    "* size_units: This is the unit of measurement of the total floor area of a house \n",
    "* lot_size: This is the total area of the land where a house is located. \n",
    "* lot_size_unit: This is the unit of measurement of the total land area \n",
    "* zip_code: This is postal address that pin-points a house\n",
    "* price: This is the amount a sold was sold, and this is the variable of interest. \n",
    "\n",
    "Important Note: Some of the unit of measurements are either in sqft or acre. For uniformity, I have used sqft for all measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = \"C:/Users/oyeni/Python Script/DecisionTreeFromScratch/Data/seattle_train.csv\"\n",
    "train = pd.read_csv(training_dataset, sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = \"C:/Users/oyeni/Python Script/DecisionTreeFromScratch/Data/seattle_test.csv\"\n",
    "test = pd.read_csv(testing_dataset, sep = ',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quick EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2016 entries, 0 to 2015\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   beds            2016 non-null   int64  \n",
      " 1   baths           2016 non-null   float64\n",
      " 2   size            2016 non-null   float64\n",
      " 3   size_units      2016 non-null   object \n",
      " 4   lot_size        1669 non-null   float64\n",
      " 5   lot_size_units  1669 non-null   object \n",
      " 6   zip_code        2016 non-null   int64  \n",
      " 7   price           2016 non-null   float64\n",
      "dtypes: float64(4), int64(2), object(2)\n",
      "memory usage: 126.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I invoked info() method on the dataset. The method returns the features and the corresponding data types. Here, zipcode is read as integer. This is not right, albeit, it appears so by just looking at the zip code column. For the purpose of this model, I changed its data type before feeding the dataset to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beds                0\n",
       "baths               0\n",
       "size                0\n",
       "size_units          0\n",
       "lot_size          347\n",
       "lot_size_units    347\n",
       "zip_code            0\n",
       "price               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "beds               0\n",
       "baths              0\n",
       "size               0\n",
       "size_units         0\n",
       "lot_size          77\n",
       "lot_size_units    77\n",
       "zip_code           0\n",
       "price              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I invoked isnull().sum() on the dataset. This method returns the dataset features and the corresponding count of missing values. lot_size and lot_size_units had missing values on both train and test datasets. This also needs to be addressed before model is trained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Unit of Measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    1449\n",
       "NaN      347\n",
       "acre     220\n",
       "Name: lot_size_units, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lot_size_units'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sqft    369\n",
       "NaN      77\n",
       "acre     59\n",
       "Name: lot_size_units, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['lot_size_units'].value_counts(dropna=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"lot_size_units\" has two different unit of measurements. When the data were collected, the data collector recorded lot size in acre and in square foot. This can be misleading to the algorithm. Applying value_counts() on the dataset with dropna=False option revealed the count of NAN a.k.a missing values, in lot_size_units.\n",
    "\n",
    "For this analysis, I have used square foot as the unit of measurement. Hence, to convert lot size recorded in acre to square foot, I multiplied the \"lot_size\" by 43560, since 1 acre equates 43560 square feet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[(train['lot_size_units'] == \"acre\"), 'lot_size'] = train['lot_size']*43560\n",
    "train['lot_size_units'] = train['lot_size_units'].str.replace(\"acre\", \"sqft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.loc[(test['lot_size_units'] == \"acre\"), 'lot_size'] = test['lot_size']*43560\n",
    "test['lot_size_units'] = test['lot_size_units'].str.replace(\"acre\", \"sqft\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing data \n",
    "\n",
    "\"lot_size\" feature had 347 and 77 missing rows in both training and testing datasets, respectively. To handle this, I replaced the missing rows with the average of the entire column. There are other ways to handle missing data, but for the purpose of this project, I decided to impute missing values with mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18789.951947273817"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['lot_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8961.000000000002"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['lot_size'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing rows in \"lot_size\" with means \n",
    "train.loc[(train['lot_size'].isnull()), 'lot_size'] = train['lot_size'].mean()\n",
    "\n",
    "# replace missing rows in \"lot_size_units\" with \"sqft\"\n",
    "train.loc[(train['lot_size_units'].isnull()), 'lot_size_units'] = 'sqft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing rows in \"lot_size\" with means \n",
    "test.loc[(test['lot_size'].isnull()), 'lot_size'] = test['lot_size'].mean()\n",
    "\n",
    "# replace missing rows in \"lot_size_units\" with \"sqft\"\n",
    "test.loc[(test['lot_size_units'].isnull()), 'lot_size_units'] = 'sqft'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Zip Code Data Type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['zip_code'] = train['zip_code'].astype(str)\n",
    "test['zip_code'] = test['zip_code'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2016 entries, 0 to 2015\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   beds            2016 non-null   int64  \n",
      " 1   baths           2016 non-null   float64\n",
      " 2   size            2016 non-null   float64\n",
      " 3   size_units      2016 non-null   object \n",
      " 4   lot_size        2016 non-null   float64\n",
      " 5   lot_size_units  2016 non-null   object \n",
      " 6   zip_code        2016 non-null   object \n",
      " 7   price           2016 non-null   float64\n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 126.1+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop('price', axis = 1).values\n",
    "Y_train = train.iloc[:, -1].values.reshape(-1,1)\n",
    "\n",
    "X_test = test.drop('price', axis = 1).values\n",
    "Y_test = test.iloc[:, -1].values.reshape(-1,1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Algorithm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of nodes in a decision tree, namely:\n",
    "\n",
    "* Decision node: contains a condition to split the data\n",
    "* Leaf node: contains the output variable which is used to make a prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left_child=None, right_child=None, var_red=None, value=None):\n",
    "        ''' constructor ''' \n",
    "        \n",
    "        # for decision node\n",
    "        self.feature_index = feature_index\n",
    "        self.threshold = threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.var_red = var_red\n",
    "        \n",
    "        # for leaf node\n",
    "        self.value = value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree class\n",
    "\n",
    "Here, I defined a class object called DecisionTreeRegressor(), just like in scikit-learn. This class contains methods that perform specific task, from tree building, splitting, to printing tree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressor():\n",
    "    def __init__(self, min_samples_split=2, max_depth=2):\n",
    "        ''' constructor '''\n",
    "        \n",
    "        # set tree foundation a.k.a root \n",
    "        self.root = None\n",
    "        \n",
    "        # set decision tree hyperparameters\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "    def build_tree(self, df, curr_depth=0):\n",
    "        ''' build tree function - this recursively builds the tree '''\n",
    "        \n",
    "        X, Y = df[:,:-1], df[:,-1]\n",
    "        sample_size, feature_size = np.shape(X)\n",
    "        best_split = {}\n",
    "        # split until stopping conditions are met\n",
    "        if sample_size>=self.min_samples_split and curr_depth<=self.max_depth:\n",
    "            # find the best split\n",
    "            best_split = self.get_best_split(df, sample_size, feature_size)\n",
    "            # check if variance reduction is positive\n",
    "            if best_split[\"var_red\"]>0:\n",
    "                # recur left\n",
    "                left_subtree = self.build_tree(best_split[\"df_left\"], curr_depth+1)\n",
    "                # recur right\n",
    "                right_subtree = self.build_tree(best_split[\"df_right\"], curr_depth+1)\n",
    "                # return decision node\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"var_red\"])\n",
    "        \n",
    "        # compute leaf node\n",
    "        leaf_value = self.calculate_leaf_value(Y)\n",
    "        # return leaf node\n",
    "        return Node(value=leaf_value)\n",
    "    \n",
    "    def get_best_split(self, df, sample_size, feature_size):\n",
    "        ''' function to find the best split '''\n",
    "        \n",
    "        # dictionary to store the best split\n",
    "        best_split = {}\n",
    "        max_var_red = -float(\"inf\")\n",
    "        # loop over all the features\n",
    "        for feature_index in range(feature_size):\n",
    "            feature_values = df[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            # loop over all the feature values present in the data\n",
    "            for threshold in possible_thresholds:\n",
    "                # get current split\n",
    "                df_left, df_right = self.split(df, feature_index, threshold)\n",
    "                # check if childs are not null\n",
    "                if len(df_left)>0 and len(df_right)>0:\n",
    "                    y, left_y, right_y = df[:, -1], df_left[:, -1], df_right[:, -1]\n",
    "                    # compute information gain >> variance reduction \n",
    "                    curr_var_red = self.variance_reduction(y, left_y, right_y)\n",
    "                    # update the best split if needed\n",
    "                    if curr_var_red>max_var_red:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"df_left\"] = df_left\n",
    "                        best_split[\"df_right\"] = df_right\n",
    "                        best_split[\"var_red\"] = curr_var_red\n",
    "                        max_var_red = curr_var_red\n",
    "                        \n",
    "        # return best split\n",
    "        return best_split\n",
    "    \n",
    "    def split(self, df, feature_index, threshold):\n",
    "        ''' function to split the data '''\n",
    "        \n",
    "        # splits data point to the left branch \n",
    "        df_left = np.array([row for row in df if row[feature_index]<=threshold])\n",
    "        # splits data point to the right branch\n",
    "        df_right = np.array([row for row in df if row[feature_index]>threshold])\n",
    "        return df_left, df_right\n",
    "    \n",
    "    def variance_reduction(self, parent, l_child, r_child):\n",
    "        ''' function to compute variance reduction -- information gain'''\n",
    "        \n",
    "        weight_l = len(l_child) / len(parent)\n",
    "        weight_r = len(r_child) / len(parent)\n",
    "        reduction = np.var(parent) - (weight_l * np.var(l_child) + weight_r * np.var(r_child))\n",
    "        return reduction\n",
    "    \n",
    "    def calculate_leaf_value(self, Y):\n",
    "        ''' function to compute predicted value at leaf node '''\n",
    "        \n",
    "        val = np.mean(Y)\n",
    "        return val\n",
    "                \n",
    "    def print_tree(self, tree=None, indent=\" \"):\n",
    "        ''' function to print the tree '''\n",
    "        \n",
    "        if not tree:\n",
    "            tree = self.root\n",
    "\n",
    "        if tree.value is not None:\n",
    "            print(tree.value)\n",
    "\n",
    "        else:\n",
    "            print(\"X_\"+str(tree.feature_index), \"<=\", tree.threshold, \"?\", tree.var_red)\n",
    "            print(\"%sleft:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.left_child, indent + indent)\n",
    "            print(\"%sright:\" % (indent), end=\"\")\n",
    "            self.print_tree(tree.right_child, indent + indent)\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        ''' function to train the tree '''\n",
    "        \n",
    "        dataset = np.concatenate((X, Y), axis=1)\n",
    "        self.root = self.build_tree(dataset)\n",
    "        \n",
    "    def make_prediction(self, x, tree):\n",
    "        ''' function to predict new dataset '''\n",
    "        \n",
    "        if tree.value!=None: return tree.value\n",
    "        feature_val = x[tree.feature_index]\n",
    "        if feature_val<=tree.threshold:\n",
    "            return self.make_prediction(x, tree.left_child)\n",
    "        else:\n",
    "            return self.make_prediction(x, tree.right_child)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ''' function to predict a single data point '''\n",
    "        \n",
    "        preditions = [self.make_prediction(x, self.root) for x in X]\n",
    "        return preditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_2 <= 2840.0 ? 110762108886.55444\n",
      " left:X_2 <= 1734.0 ? 41906843701.24878\n",
      "  left:X_0 <= 1 ? 14730735665.033264\n",
      "    left:X_2 <= 1250.0 ? 12954779282.883347\n",
      "        left:X_2 <= 781.0 ? 4176807931.641815\n",
      "                left:X_2 <= 460.0 ? 6095117392.498131\n",
      "                                left:822000.0\n",
      "                                right:401305.15873015876\n",
      "                right:X_2 <= 945.0 ? 4404529021.937183\n",
      "                                left:534457.1282051282\n",
      "                                right:684989.2857142857\n",
      "        right:X_4 <= 41817.6 ? 199666805555.55557\n",
      "                left:X_6 <= 98105 ? 30450250000.0\n",
      "                                left:1350000.0\n",
      "                                right:913750.0\n",
      "                right:2200000.0\n",
      "    right:X_2 <= 730.0 ? 22497061648.7594\n",
      "        left:X_2 <= 724.0 ? 13445435929390.162\n",
      "                left:X_4 <= 18789.951947273817 ? 11815231666.765432\n",
      "                                left:523464.0\n",
      "                                right:998000.0\n",
      "                right:X_4 <= 5150.0 ? 149511756250000.0\n",
      "                                left:545000.0\n",
      "                                right:25000000.0\n",
      "        right:X_2 <= 1250.0 ? 8705137529.320038\n",
      "                left:X_4 <= 6350.0 ? 2752611819.188099\n",
      "                                left:689827.0720720721\n",
      "                                right:584739.4522613066\n",
      "                right:X_6 <= 98102 ? 7703303140.580818\n",
      "                                left:1552500.0\n",
      "                                right:816602.6066945606\n",
      "  right:X_6 <= 98105 ? 41437458166.351685\n",
      "    left:X_1 <= 1.0 ? 813458631923.3613\n",
      "        left:X_2 <= 1800.0 ? 31875188606805.562\n",
      "                left:X_2 <= 1770.0 ? 144889369000000.0\n",
      "                                left:926000.0\n",
      "                                right:25000000.0\n",
      "                right:X_4 <= 3726.0 ? 23157260208.333332\n",
      "                                left:898566.6666666666\n",
      "                                right:1250000.0\n",
      "        right:X_1 <= 4.5 ? 289041081118.98267\n",
      "                left:X_6 <= 98101 ? 28450272602.0343\n",
      "                                left:2389000.0\n",
      "                                right:1295554.6625\n",
      "                right:6250000.0\n",
      "    right:X_2 <= 2340.0 ? 13277393308.668137\n",
      "        left:X_6 <= 98122 ? 8566502984.978546\n",
      "                left:X_6 <= 98106 ? 6802267368.02182\n",
      "                                left:780545.0\n",
      "                                right:1063301.6321243523\n",
      "                right:X_1 <= 1.5 ? 5682328199.52195\n",
      "                                left:690368.0\n",
      "                                right:882754.7663551401\n",
      "        right:X_1 <= 2.0 ? 12590672845.184082\n",
      "                left:X_1 <= 1.0 ? 12196429255.616745\n",
      "                                left:705833.3333333334\n",
      "                                right:1102880.7692307692\n",
      "                right:X_0 <= 4 ? 13876235539.685013\n",
      "                                left:1365218.3827160494\n",
      "                                right:1090561.5384615385\n",
      " right:X_2 <= 4018.0 ? 211506126783.0498\n",
      "  left:X_1 <= 3.0 ? 50398564258.714905\n",
      "    left:X_4 <= 8360.0 ? 49151048011.79602\n",
      "        left:X_6 <= 98112 ? 37220584648.440186\n",
      "                left:X_0 <= 3 ? 98518440833.33334\n",
      "                                left:2412000.0\n",
      "                                right:1687133.3333333333\n",
      "                right:X_2 <= 3400.0 ? 27670618520.44574\n",
      "                                left:1348383.7837837837\n",
      "                                right:1735208.3333333333\n",
      "        right:X_4 <= 32670.0 ? 61669649664.463135\n",
      "                left:X_6 <= 98144 ? 60797168533.389694\n",
      "                                left:1146250.0\n",
      "                                right:651642.8571428572\n",
      "                right:X_1 <= 2.0 ? 330213555555.5555\n",
      "                                left:703500.0\n",
      "                                right:1922500.0\n",
      "    right:X_0 <= 6 ? 46311679659.132996\n",
      "        left:X_2 <= 3530.0 ? 43940898243.290405\n",
      "                left:X_4 <= 16988.4 ? 37434602296.00336\n",
      "                                left:1791271.875\n",
      "                                right:2482416.6666666665\n",
      "                right:X_4 <= 5259.0 ? 100408007812.5\n",
      "                                left:2779375.0\n",
      "                                right:2107187.5\n",
      "        right:X_6 <= 98116 ? 26008344000.0\n",
      "                left:X_0 <= 8 ? 7080500000.0\n",
      "                                left:1500000.0\n",
      "                                right:1321500.0\n",
      "                right:X_2 <= 3014.0 ? 11329473600.0\n",
      "                                left:835000.0\n",
      "                                right:1101100.0\n",
      "  right:X_4 <= 5880.0 ? 237866905142.26282\n",
      "    left:X_6 <= 98119 ? 203414003155.8185\n",
      "        left:X_1 <= 4.0 ? 126736474285.71426\n",
      "                left:X_1 <= 3.0 ? 215407360544.2177\n",
      "                                left:1650000.0\n",
      "                                right:2976333.3333333335\n",
      "                right:X_2 <= 4260.0 ? 42050000000.0\n",
      "                                left:2300000.0\n",
      "                                right:1865000.0\n",
      "        right:X_2 <= 4490.0 ? 55555555555.55556\n",
      "                left:1150000.0\n",
      "                right:X_0 <= 4 ? 10000000000.0\n",
      "                                left:1750000.0\n",
      "                                right:1550000.0\n",
      "    right:X_2 <= 4040.0 ? 214601203899.25427\n",
      "        left:5495000.0\n",
      "        right:X_2 <= 4693.0 ? 225874868002.12573\n",
      "                left:X_4 <= 6867.0 ? 144450725994.1505\n",
      "                                left:3283333.3333333335\n",
      "                                right:2453959.714285714\n",
      "                right:X_2 <= 5090.0 ? 166622512500.0\n",
      "                                left:4570000.0\n",
      "                                right:3474700.0\n"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeRegressor(min_samples_split=2, max_depth=5)\n",
    "model.fit(X_train, Y_train)\n",
    "model.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to estimate root mean square error\n",
    "def rmse(actual, pred, X_data):\n",
    "    Y_pred = model.predict(X_data)\n",
    "    MSE = np.square(np.subtract(actual,pred)).mean()\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    return RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "827339.3610334604"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction on validation dataset\n",
    "rmse(Y_test, Y_pred, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098542.476033523"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Prediction on training dataset\n",
    "rmse(Y_train, Y_pred, X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila, there you have it!! It is obvious the algorithm didn't generalize well to a new dataset, as the root mean square error (RMSE) on the test dataset is significantly high. The goal here is to optimize the loss function such that RMSE is close to zero. The RMSE on the training set is also high, which simply reveals that the algorithm is not anywhere near a production algorithm. There are couple of ways this algorithm can be improved, to mention a few:\n",
    "\n",
    "* Increase the training dataset\n",
    "* Add more relevant features \n",
    "* Apply the concept of hyperparameter tunning to choose the best hyperparameters for the algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
